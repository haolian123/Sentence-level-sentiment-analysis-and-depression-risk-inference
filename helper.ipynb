{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该文件是一个赋值文件，用于计算一些值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算阈值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算结果1的阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果1的阈值为: 44.82722222222222\n"
     ]
    }
   ],
   "source": [
    "#计算结果一的阈值\n",
    "from MyModel import Classification\n",
    "from HaoChiUtils import DataAnalyzer as DA\n",
    "import os\n",
    "def get_result1_threshold(file_path):\n",
    "    label_list=['快乐','恐惧','愤怒','惊讶','喜爱','厌恶','悲伤']\n",
    "    \n",
    "    weight_list=[-1,0.5,1,0,-1.5,0.5,1.3]\n",
    "    # 初始化模型\n",
    "    myClassification=Classification(\"bert_model\")\n",
    "    # 读取文本\n",
    "    fir_list=os.listdir(file_path)\n",
    "\n",
    "    total_result1=0\n",
    "    for i in fir_list:\n",
    "        data = DA.get_dataList(file_path+'/'+i,min_len=6)  # 获取文本数据\n",
    "        # 预测\n",
    "        pre=myClassification.get_predict_result(data)  # 使用模型进行预测\n",
    "        res_dict=DA.calculate_label_proportions(pre,label_list=label_list)  # 计算预测结果中各标签的比例\n",
    "        result1=0\n",
    "        # print(\"res_dict：\",res_dict)\n",
    "        for i in range(len(label_list)):\n",
    "            result1=result1+res_dict[label_list[i]]*weight_list[i]*100  # 根据权重计算结果\n",
    "        total_result1+=result1\n",
    "    return total_result1/len(fir_list)*0.7  # 返回平均结果\n",
    "filepath=\"用于计算结果1阈值和熵率阈值的用户文本\"\n",
    "res=get_result1_threshold(filepath)\n",
    "print(\"结果1的阈值为:\",res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算熵率的阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "熵率的阈值 =  0.08716504273252149\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from MyModel import Classification\n",
    "from HaoChiUtils import DataAnalyzer as DA\n",
    "import os\n",
    "#计算判断熵率高低的阈值\n",
    "def get_Entropy_threshold(file_path=\"用于计算结果1阈值和熵率阈值的用户文本\"):\n",
    "    label_list=['快乐','恐惧','愤怒','惊讶','喜爱','厌恶','悲伤']\n",
    "    label_dict = {label: 0 for label in label_list}\n",
    "    # 初始化模型\n",
    "    myClassification=Classification(\"bert_model\")\n",
    "    # 读取文本\n",
    "    fir_list=os.listdir(file_path)\n",
    "    # total_result1=0\n",
    "    for i in fir_list:\n",
    "        data = DA.get_dataList(file_path+'/'+i,min_len=6)  # 获取文本数据\n",
    "        # 预测\n",
    "        pre=myClassification.get_predict_result(data)  # 使用模型进行预测\n",
    "        res_dict=DA.calculate_label_proportions(pre,label_list=label_list)  # 计算预测结果中各标签的比例\n",
    "        # result1=0\n",
    "        # print(\"res_dict：\",res_dict)\n",
    "        for i in range(len(label_list)):\n",
    "            label_dict[label_list[i]]+=res_dict[label_list[i]]\n",
    "        # total_result1+=result1\n",
    "    for i in label_dict.keys():\n",
    "        label_dict[i]=label_dict[i]/(len(fir_list))\n",
    "    total_pi=0\n",
    "    for i in label_list:\n",
    "        if i =='惊讶':\n",
    "            continue\n",
    "        pi=label_dict[i]\n",
    "        total_pi+=-pi*math.log2(pi)\n",
    "    return total_pi/20\n",
    "filepath=\"用于计算结果1阈值和熵率阈值的用户文本\"\n",
    "res=get_Entropy_threshold(filepath)\n",
    "print(\"熵率的阈值 = \",res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算情绪变化的阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "情绪变化的阈值= 0.35\n"
     ]
    }
   ],
   "source": [
    "#计算情绪变化阈值\n",
    "from FunctionalInterface import TextEmotionAnalyzer as TEA \n",
    "import os\n",
    "import math\n",
    "#求最大最小值\n",
    "def maxmin(pro_list):\n",
    "    Max=0\n",
    "    Min=1\n",
    "    for i in pro_list:\n",
    "        if i >0 and i<Min:\n",
    "            Min=i\n",
    "        if i>Max:\n",
    "            Max=i \n",
    "    return Max,Min\n",
    "#归一化,传入一个字典\n",
    "def calculate_mood_change_threshold(pro_dict):\n",
    "    # pro_keys=pro_dict.keys()\n",
    "    pro_values=pro_dict.values()\n",
    "    pro_max,pro_min=maxmin(pro_values)\n",
    "    div=pro_max-pro_min\n",
    "    if div ==0:\n",
    "        div=1\n",
    "    pro_values=[max((x-pro_min)/div,0) for x in pro_values]\n",
    "    mean_u=sum(pro_values)/len(pro_values)\n",
    "    S_square=0\n",
    "    x_sum=0\n",
    "    for x in pro_values:\n",
    "        x_sum+=(x-mean_u)*(x-mean_u)\n",
    "    S_square=x_sum/len(pro_values)\n",
    "    S=round(math.sqrt(S_square),2)\n",
    "    return S\n",
    "#开始计算\n",
    "tea=TEA()\n",
    "res_S=[]\n",
    "folder_path=\"用于计算情绪变化阈值的用户\"\n",
    "file_list=os.listdir(folder_path)\n",
    "for user in file_list:\n",
    "    pro_dict=tea.emotions_proportion(f\"{folder_path}\\\\{user}\",min_len=6) \n",
    "    S=calculate_mood_change_threshold(pro_dict=pro_dict)\n",
    "    res_S.append(S)\n",
    "mood_change_threshold=round(sum(res_S)/len(res_S),4)\n",
    "print(\"情绪变化的阈值=\",mood_change_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算最终评价风险等级的阈值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#平均值\n",
    "#传入一个二维列表，是每个用户的风险等级列表\n",
    "def reverse_rows(matrix):\n",
    "    return [row[::-1] for row in matrix]\n",
    "#计算一个用户前month个月风险平均值\n",
    "def risk_mean(risk_list,month):\n",
    "    if(len(risk_list)<month):\n",
    "        return -1\n",
    "    risk_sum=0\n",
    "    Len=month \n",
    "    for i in range(Len):\n",
    "        risk_sum+=risk_list[i]\n",
    "    \n",
    "    return round(risk_sum/Len,4)\n",
    "#根据月份，计算所有用户前month个月风险平均值\n",
    "def cal_mean(risks,month):\n",
    "    risk_month=[]\n",
    "    for risk in risks:\n",
    "        risk_month.append(risk_mean(risk,month))\n",
    "    cnt=0\n",
    "    _sum=0\n",
    "    # print(risk_month)\n",
    "    for i in risk_month:\n",
    "        if i != -1:\n",
    "            cnt+=1\n",
    "            _sum+=i\n",
    "    return round(_sum/cnt,4)\n",
    "#得到 近 3 6 9 个月的风险等级平均值\n",
    "def get_mean_threshold(risk_list):\n",
    "    #按时间逆序\n",
    "    risks=reverse_rows(risk_list)\n",
    "    mean_3=cal_mean(risks=risks,month=3)\n",
    "    mean_6=cal_mean(risks=risks,month=6)\n",
    "    mean_9=cal_mean(risks=risks,month=9)\n",
    "    return mean_3,mean_6,mean_9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#标准差\n",
    "#得到每个用户 3、6、9 月的标准差\n",
    "#接着计算得到3、6、9平均标准差 三个标准差阈值\n",
    "import math\n",
    "#传入一个二维列表，是每个用户的风险等级列表\n",
    "def reverse_rows(matrix):\n",
    "    return [row[::-1] for row in matrix]\n",
    "#计算一个用户前month个月风险平均值\n",
    "def risk_S(risk_list,month):\n",
    "    #月份不够、跳过\n",
    "    if(len(risk_list)<month):\n",
    "        return -1\n",
    "    risk_sum=0\n",
    "    Len=month \n",
    "    for i in range(Len):\n",
    "        risk_sum+=risk_list[i]\n",
    "    #得到平均值\n",
    "    x_mean=risk_sum/Len \n",
    "    S_squre=0\n",
    "    for i in range(Len):\n",
    "        S_squre+=(risk_list[i]-x_mean)*(risk_list[i]-x_mean)\n",
    "    return math.sqrt(S_squre/Len)\n",
    "    \n",
    "\n",
    "\n",
    "#根据月份，计算所有用户前month个月风险平均标准差\n",
    "def cal_S(risks,month):\n",
    "    risk_month=[]\n",
    "    for risk in risks:\n",
    "        risk_month.append(risk_S(risk,month))\n",
    "    cnt=0\n",
    "    _sum=0\n",
    "    # print(risk_month)\n",
    "    for i in risk_month:\n",
    "        if i != -1:\n",
    "            cnt+=1\n",
    "            _sum+=i\n",
    "    return round(_sum/cnt,4)\n",
    "def get_S_threshold(risk_list):\n",
    "    #按时间逆序\n",
    "    risks=reverse_rows(risk_list)\n",
    "    S_3=cal_S(risks=risks,month=3)\n",
    "    S_6=cal_S(risks=risks,month=6)\n",
    "    S_9=cal_S(risks=risks,month=9)\n",
    "    return S_3,S_6,S_9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "近3、6、9个月风险等级平均值阈值分别为：0.7719、1.0238、0.9012\n",
      "近3、6、9个月风险等级平均标准差阈值分别为：0.81、0.8342、0.8582\n"
     ]
    }
   ],
   "source": [
    "#根据抑郁风险用户得到阈值\n",
    "from FunctionalInterface import TextEmotionAnalyzer as TEA \n",
    "tea=TEA()\n",
    "import os\n",
    "folder_path=\"用于计算风险标准差和平均值阈值的用户文本\"\n",
    "def get_risk_lists(folder_path):\n",
    "    users=os.listdir(folder_path)\n",
    "    risk_lists=[]\n",
    "    for user in users:\n",
    "        src_folder_path=folder_path+'\\\\'+user\n",
    "        risk_lists.append(tea.risk_rank_list(src_folder_path=src_folder_path,min_len=6))\n",
    "    return risk_lists\n",
    "risk_list=get_risk_lists(folder_path)\n",
    "mean_3,mean_6,mean_9=get_mean_threshold(risk_list=risk_list)\n",
    "S_3,S_6,S_9=get_S_threshold(risk_list=risk_list)\n",
    "print(rf\"近3、6、9个月风险等级平均值阈值分别为：{mean_3}、{mean_6}、{mean_9}\")\n",
    "print(rf\"近3、6、9个月风险等级平均标准差阈值分别为：{S_3}、{S_6}、{S_9}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_weighting(score_3,score_6,score_9):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成词云\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "万迪羊肉火锅\n",
      "今渡百川\n",
      "仙泷小柒_\n",
      "低级普男\n",
      "你的镜仔气网友版\n",
      "冒牌写手\n",
      "我脑袋瓜嗡嗡的\n",
      "扛不住墙头马上大火\n",
      "是林需药\n",
      "猫猫张圆圆\n",
      "绝对不能被发现的微博小号\n",
      "都好说但是要先给钱\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from wordcloud import WordCloud\n",
    "from HaoChiUtils import DataPreprocess as DP\n",
    "from collections import Counter\n",
    "import os\n",
    "dp=DP(stopwords_file_path=\"stop_words.txt\")\n",
    "def get_wordcloud(folder_path):\n",
    "    folders_list=os.listdir(folder_path)\n",
    "    strs=\" \"\n",
    "    for folder in folders_list:\n",
    "        user_text_list=os.listdir(folder_path+'\\\\'+folder)\n",
    "        print(folder)\n",
    "        for user_text in user_text_list:\n",
    "\n",
    "            file_path=folder_path+'\\\\'+folder+'\\\\'+user_text\n",
    "            with open(file_path,'r',encoding='utf-8') as f:\n",
    "                cnt=0\n",
    "                for line in f:\n",
    "                    cnt+=1\n",
    "                    text=dp.text_clean(line,keep_segmentation=False)\n",
    "                    text=text.split(' ')\n",
    "                    text=[i for i in text if i!='' and len(i)>=2]\n",
    "                    # print(text)\n",
    "                    strs+=' '.join(text)\n",
    "    return strs\n",
    "folder_path=\"制作词云\\正常用户\"\n",
    "seg_text=get_wordcloud(folder_path)\n",
    "# seg_list=seg_text.split(' ')\n",
    "# word_freq = Counter(seg_list)\n",
    "# top_n=50\n",
    "# top_words=dict(word_freq.most_common(top_n))\n",
    "# # 创建词云对象并生成词云\n",
    "# wordcloud = WordCloud(font_path='myttf.ttf', width=800, height=400,collocations=False).generate_from_frequencies(top_words)\n",
    "\n",
    "# # 显示词云\n",
    "# wordcloud.to_image()\n",
    "\n",
    "# # 显示词云\n",
    "# # wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "color=[\"Blues\",\"Reds\",\"Greens\",\"Oranges\",\"Purples\",\"RdYlBu\",\"viridis\",'spring','summer','autumn','winter']\n",
    "seg_list=seg_text.split(' ')\n",
    "word_freq = Counter(seg_list)\n",
    "top_n=80\n",
    "top_words=dict(word_freq.most_common(top_n))\n",
    "# 创建词云对象并生成词云\n",
    "# wordcloud = WordCloud(font_path='FZYTK.ttf',background_color='white', width=800, height=400,collocations=True,colormap='summer',color_func=lambda *args, **kwargs: \"black\").generate_from_frequencies(top_words)\n",
    "for i in range(len(color)):\n",
    "    wordcloud = WordCloud(font_path='myttf.ttf',background_color='black', width=1000, height=600,collocations=True,colormap=color[i]).generate_from_frequencies(top_words)\n",
    "\n",
    "    # 显示词云\n",
    "    # wordcloud.to_image()\n",
    "\n",
    "    # 显示词云\n",
    "    # wordcloud.to_image()\n",
    "    wordcloud.to_file(f\"词云\\正常用户\\\\{i+1}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# wd_0=WordCloud(font_path='myttf.TTF',\n",
    "#                colormap='viridis',\n",
    "#                 width=800,\n",
    "#                 height=400,\n",
    "#                collocations=False\n",
    "#                ).generate(seg_text)\n",
    "# plt.imshow(wd_0,interpolation='bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #将一组文本整合成一个\n",
    "# folder_path=\"用于计算阈值的用户\"\n",
    "# fir_dir=os.listdir(folder_path)\n",
    "# for user in fir_dir:\n",
    "#     with open(f\"{folder_path}\\\\{user}.txt\",'w',encoding='utf-8') as f_write:\n",
    "#         user_path=folder_path+'\\\\'+user \n",
    "#         print(user_path)\n",
    "#         months=os.listdir(user_path)\n",
    "#         for month in months:\n",
    "#             user_text_path=user_path+'\\\\'+month\n",
    "#             with open(user_text_path,'r',encoding='utf-8') as f_read:\n",
    "#                 for line in f_read:\n",
    "#                     line=line.strip()\n",
    "#                     f_write.write(line+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\12631\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\paddle\\nn\\layer\\layers.py:1897: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\n",
      "c:\\Users\\12631\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\paddle\\nn\\layer\\layers.py:1897: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\n",
      "c:\\Users\\12631\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:2287: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\12631\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:1859: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ammm18_20230117 20230629 .txt\n",
      "{'快乐': 0.02, '恐惧': 0.0, '愤怒': 0.09, '惊讶': 0.04, '喜爱': 0.02, '厌恶': 0.06, '悲伤': 0.77}\n",
      "hhfxvbhxtjcckf_20230131 20230629 .txt\n",
      "{'快乐': 0.05, '恐惧': 0.02, '愤怒': 0.02, '惊讶': 0.0, '喜爱': 0.06, '厌恶': 0.01, '悲伤': 0.85}\n",
      "icouldnotrunsoiflewaway_20230227 20230711 .txt\n",
      "{'快乐': 0.08, '恐惧': 0.01, '愤怒': 0.07, '惊讶': 0.0, '喜爱': 0.03, '厌恶': 0.06, '悲伤': 0.75}\n",
      "JHw21-_20230421 20230628 .txt\n",
      "{'快乐': 0.16, '恐惧': 0.0, '愤怒': 0.0, '惊讶': 0.02, '喜爱': 0.02, '厌恶': 0.07, '悲伤': 0.72}\n",
      "shsjxhdjxjd_20220726 20230731 .txt\n",
      "{'快乐': 0.13, '恐惧': 0.01, '愤怒': 0.04, '惊讶': 0.0, '喜爱': 0.07, '厌恶': 0.01, '悲伤': 0.73}\n",
      "S_u_bmerge_20230201 20230717 .txt\n",
      "{'快乐': 0.06, '恐惧': 0.0, '愤怒': 0.12, '惊讶': 0.03, '喜爱': 0.03, '厌恶': 0.03, '悲伤': 0.72}\n",
      "WHysoSeErious.txt\n",
      "{'快乐': 0.17, '恐惧': 0.0, '愤怒': 0.17, '惊讶': 0.0, '喜爱': 0.25, '厌恶': 0.0, '悲伤': 0.42}\n",
      "_壹然__20230407 20230721 .txt\n",
      "{'快乐': 0.12, '恐惧': 0.02, '愤怒': 0.06, '惊讶': 0.0, '喜爱': 0.05, '厌恶': 0.13, '悲伤': 0.61}\n",
      "东城不下雨.txt\n",
      "{'快乐': 0.26, '恐惧': 0.0, '愤怒': 0.23, '惊讶': 0.0, '喜爱': 0.1, '厌恶': 0.1, '悲伤': 0.32}\n",
      "九八年生人_20230118 20230715 .txt\n",
      "{'快乐': 0.1, '恐惧': 0.07, '愤怒': 0.03, '惊讶': 0.02, '喜爱': 0.05, '厌恶': 0.05, '悲伤': 0.69}\n",
      "九月将尽方初始.txt\n",
      "{'快乐': 0.22, '恐惧': 0.01, '愤怒': 0.01, '惊讶': 0.0, '喜爱': 0.19, '厌恶': 0.0, '悲伤': 0.56}\n",
      "是优秀的饲养员吖.txt\n",
      "{'快乐': 0.2, '恐惧': 0.0, '愤怒': 0.12, '惊讶': 0.0, '喜爱': 0.12, '厌恶': 0.05, '悲伤': 0.51}\n",
      "用户7814874451_20230128 20230718 .txt\n",
      "{'快乐': 0.16, '恐惧': 0.03, '愤怒': 0.09, '惊讶': 0.0, '喜爱': 0.03, '厌恶': 0.12, '悲伤': 0.56}\n",
      "用户7836765553_20230504 20230707 .txt\n",
      "{'快乐': 0.18, '恐惧': 0.0, '愤怒': 0.21, '惊讶': 0.0, '喜爱': 0.04, '厌恶': 0.0, '悲伤': 0.57}\n",
      "而代价是无痛的爱_20221025 20230318 .txt\n",
      "{'快乐': 0.11, '恐惧': 0.0, '愤怒': 0.14, '惊讶': 0.0, '喜爱': 0.1, '厌恶': 0.14, '悲伤': 0.51}\n",
      "西瓜丸奶三分糖少冰.txt\n",
      "{'快乐': 0.34, '恐惧': 0.0, '愤怒': 0.0, '惊讶': 0.02, '喜爱': 0.12, '厌恶': 0.02, '悲伤': 0.51}\n",
      "要紫鲨_20230125 20230621 .txt\n",
      "{'快乐': 0.07, '恐惧': 0.01, '愤怒': 0.06, '惊讶': 0.0, '喜爱': 0.07, '厌恶': 0.03, '悲伤': 0.75}\n",
      "韵雾就是我_重拾快乐版.txt\n",
      "{'快乐': 0.27, '恐惧': 0.0, '愤怒': 0.18, '惊讶': 0.01, '喜爱': 0.11, '厌恶': 0.05, '悲伤': 0.39}\n"
     ]
    }
   ],
   "source": [
    "from HaoChiUtils import DataAnalyzer as DA\n",
    "from FunctionalInterface import TextEmotionAnalyzer as TEA\n",
    "import os\n",
    "#计算得到占比\n",
    "tea=TEA()\n",
    "file_path=\"计算各项阈值的文本\\用于计算结果1阈值和熵率阈值的用户文本\"\n",
    "users=os.listdir(file_path)\n",
    "for user in users:\n",
    "    user_path=file_path+'\\\\'+user\n",
    "    data_list=DA.get_dataList(user_path,min_len=6)\n",
    "    pro_dict=tea.dri.get_pro_dict(data_list)\n",
    "    print(user)\n",
    "    print(pro_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
