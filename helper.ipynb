{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该文件是一个赋值文件，用于计算一些值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算阈值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算结果1的阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果1的阈值为: 44.82722222222222\n"
     ]
    }
   ],
   "source": [
    "#计算结果一的阈值\n",
    "from MyModel import Classification\n",
    "from HaoChiUtils import DataAnalyzer as DA\n",
    "import os\n",
    "def get_result1_threshold(file_path):\n",
    "    label_list=['快乐','恐惧','愤怒','惊讶','喜爱','厌恶','悲伤']\n",
    "    \n",
    "    weight_list=[-1,0.5,1,0,-1.5,0.5,1.3]\n",
    "    # 初始化模型\n",
    "    myClassification=Classification(\"bert_model\")\n",
    "    # 读取文本\n",
    "    fir_list=os.listdir(file_path)\n",
    "\n",
    "    total_result1=0\n",
    "    for i in fir_list:\n",
    "        data = DA.get_dataList(file_path+'/'+i,min_len=6)  # 获取文本数据\n",
    "        # 预测\n",
    "        pre=myClassification.get_predict_result(data)  # 使用模型进行预测\n",
    "        res_dict=DA.calculate_label_proportions(pre,label_list=label_list)  # 计算预测结果中各标签的比例\n",
    "        result1=0\n",
    "        # print(\"res_dict：\",res_dict)\n",
    "        for i in range(len(label_list)):\n",
    "            result1=result1+res_dict[label_list[i]]*weight_list[i]*100  # 根据权重计算结果\n",
    "        total_result1+=result1\n",
    "    return total_result1/len(fir_list)*0.7  # 返回平均结果\n",
    "filepath=\"用于计算结果1阈值和熵率阈值的用户文本\"\n",
    "res=get_result1_threshold(filepath)\n",
    "print(\"结果1的阈值为:\",res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算熵率的阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "熵率的阈值 =  0.08716504273252149\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from MyModel import Classification\n",
    "from HaoChiUtils import DataAnalyzer as DA\n",
    "import os\n",
    "#计算判断熵率高低的阈值\n",
    "def get_Entropy_threshold(file_path=\"用于计算结果1阈值和熵率阈值的用户文本\"):\n",
    "    label_list=['快乐','恐惧','愤怒','惊讶','喜爱','厌恶','悲伤']\n",
    "    label_dict = {label: 0 for label in label_list}\n",
    "    # 初始化模型\n",
    "    myClassification=Classification(\"bert_model\")\n",
    "    # 读取文本\n",
    "    fir_list=os.listdir(file_path)\n",
    "    # total_result1=0\n",
    "    for i in fir_list:\n",
    "        data = DA.get_dataList(file_path+'/'+i,min_len=6)  # 获取文本数据\n",
    "        # 预测\n",
    "        pre=myClassification.get_predict_result(data)  # 使用模型进行预测\n",
    "        res_dict=DA.calculate_label_proportions(pre,label_list=label_list)  # 计算预测结果中各标签的比例\n",
    "        # result1=0\n",
    "        # print(\"res_dict：\",res_dict)\n",
    "        for i in range(len(label_list)):\n",
    "            label_dict[label_list[i]]+=res_dict[label_list[i]]\n",
    "        # total_result1+=result1\n",
    "    for i in label_dict.keys():\n",
    "        label_dict[i]=label_dict[i]/(len(fir_list))\n",
    "    total_pi=0\n",
    "    for i in label_list:\n",
    "        if i =='惊讶':\n",
    "            continue\n",
    "        pi=label_dict[i]\n",
    "        total_pi+=-pi*math.log2(pi)\n",
    "    return total_pi/20\n",
    "filepath=\"用于计算结果1阈值和熵率阈值的用户文本\"\n",
    "res=get_Entropy_threshold(filepath)\n",
    "print(\"熵率的阈值 = \",res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算情绪变化的阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "情绪变化的阈值= 0.35\n"
     ]
    }
   ],
   "source": [
    "#计算情绪变化阈值\n",
    "from FunctionalInterface import TextEmotionAnalyzer as TEA \n",
    "import os\n",
    "import math\n",
    "#求最大最小值\n",
    "def maxmin(pro_list):\n",
    "    Max=0\n",
    "    Min=1\n",
    "    for i in pro_list:\n",
    "        if i >0 and i<Min:\n",
    "            Min=i\n",
    "        if i>Max:\n",
    "            Max=i \n",
    "    return Max,Min\n",
    "#归一化,传入一个字典\n",
    "def calculate_mood_change_threshold(pro_dict):\n",
    "    # pro_keys=pro_dict.keys()\n",
    "    pro_values=pro_dict.values()\n",
    "    pro_max,pro_min=maxmin(pro_values)\n",
    "    div=pro_max-pro_min\n",
    "    if div ==0:\n",
    "        div=1\n",
    "    pro_values=[max((x-pro_min)/div,0) for x in pro_values]\n",
    "    mean_u=sum(pro_values)/len(pro_values)\n",
    "    S_square=0\n",
    "    x_sum=0\n",
    "    for x in pro_values:\n",
    "        x_sum+=(x-mean_u)*(x-mean_u)\n",
    "    S_square=x_sum/len(pro_values)\n",
    "    S=round(math.sqrt(S_square),2)\n",
    "    return S\n",
    "#开始计算\n",
    "tea=TEA()\n",
    "res_S=[]\n",
    "folder_path=\"用于计算情绪变化阈值的用户\"\n",
    "file_list=os.listdir(folder_path)\n",
    "for user in file_list:\n",
    "    pro_dict=tea.emotions_proportion(f\"{folder_path}\\\\{user}\",min_len=6) \n",
    "    S=calculate_mood_change_threshold(pro_dict=pro_dict)\n",
    "    res_S.append(S)\n",
    "mood_change_threshold=round(sum(res_S)/len(res_S),4)\n",
    "print(\"情绪变化的阈值=\",mood_change_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算最终评价风险等级的阈值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#平均值\n",
    "#传入一个二维列表，是每个用户的风险等级列表\n",
    "def reverse_rows(matrix):\n",
    "    return [row[::-1] for row in matrix]\n",
    "#计算一个用户前month个月风险平均值\n",
    "def risk_mean(risk_list,month):\n",
    "    if(len(risk_list)<month):\n",
    "        return -1\n",
    "    risk_sum=0\n",
    "    Len=month \n",
    "    for i in range(Len):\n",
    "        risk_sum+=risk_list[i]\n",
    "    \n",
    "    return round(risk_sum/Len,4)\n",
    "#根据月份，计算所有用户前month个月风险平均值\n",
    "def cal_mean(risks,month):\n",
    "    risk_month=[]\n",
    "    for risk in risks:\n",
    "        risk_month.append(risk_mean(risk,month))\n",
    "    cnt=0\n",
    "    _sum=0\n",
    "    # print(risk_month)\n",
    "    for i in risk_month:\n",
    "        if i != -1:\n",
    "            cnt+=1\n",
    "            _sum+=i\n",
    "    return round(_sum/cnt,4)\n",
    "#得到 近 3 6 9 个月的风险等级平均值\n",
    "def get_mean_threshold(risk_list):\n",
    "    #按时间逆序\n",
    "    risks=reverse_rows(risk_list)\n",
    "    mean_3=cal_mean(risks=risks,month=3)\n",
    "    mean_6=cal_mean(risks=risks,month=6)\n",
    "    mean_9=cal_mean(risks=risks,month=9)\n",
    "    return mean_3,mean_6,mean_9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#标准差\n",
    "#得到每个用户 3、6、9 月的标准差\n",
    "#接着计算得到3、6、9平均标准差 三个标准差阈值\n",
    "import math\n",
    "#传入一个二维列表，是每个用户的风险等级列表\n",
    "def reverse_rows(matrix):\n",
    "    return [row[::-1] for row in matrix]\n",
    "#计算一个用户前month个月风险平均值\n",
    "def risk_S(risk_list,month):\n",
    "    #月份不够、跳过\n",
    "    if(len(risk_list)<month):\n",
    "        return -1\n",
    "    risk_sum=0\n",
    "    Len=month \n",
    "    for i in range(Len):\n",
    "        risk_sum+=risk_list[i]\n",
    "    #得到平均值\n",
    "    x_mean=risk_sum/Len \n",
    "    S_squre=0\n",
    "    for i in range(Len):\n",
    "        S_squre+=(risk_list[i]-x_mean)*(risk_list[i]-x_mean)\n",
    "    return math.sqrt(S_squre/Len)\n",
    "    \n",
    "\n",
    "\n",
    "#根据月份，计算所有用户前month个月风险平均标准差\n",
    "def cal_S(risks,month):\n",
    "    risk_month=[]\n",
    "    for risk in risks:\n",
    "        risk_month.append(risk_S(risk,month))\n",
    "    cnt=0\n",
    "    _sum=0\n",
    "    # print(risk_month)\n",
    "    for i in risk_month:\n",
    "        if i != -1:\n",
    "            cnt+=1\n",
    "            _sum+=i\n",
    "    return round(_sum/cnt,4)\n",
    "def get_S_threshold(risk_list):\n",
    "    #按时间逆序\n",
    "    risks=reverse_rows(risk_list)\n",
    "    S_3=cal_S(risks=risks,month=3)\n",
    "    S_6=cal_S(risks=risks,month=6)\n",
    "    S_9=cal_S(risks=risks,month=9)\n",
    "    return S_3,S_6,S_9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "近3、6、9个月风险等级平均值阈值分别为：0.7719、1.0238、0.9012\n",
      "近3、6、9个月风险等级平均标准差阈值分别为：0.81、0.8342、0.8582\n"
     ]
    }
   ],
   "source": [
    "#根据抑郁风险用户得到阈值\n",
    "from FunctionalInterface import TextEmotionAnalyzer as TEA \n",
    "tea=TEA()\n",
    "import os\n",
    "folder_path=\"用于计算风险标准差和平均值阈值的用户文本\"\n",
    "def get_risk_lists(folder_path):\n",
    "    users=os.listdir(folder_path)\n",
    "    risk_lists=[]\n",
    "    for user in users:\n",
    "        src_folder_path=folder_path+'\\\\'+user\n",
    "        risk_lists.append(tea.risk_rank_list(src_folder_path=src_folder_path,min_len=6))\n",
    "    return risk_lists\n",
    "risk_list=get_risk_lists(folder_path)\n",
    "mean_3,mean_6,mean_9=get_mean_threshold(risk_list=risk_list)\n",
    "S_3,S_6,S_9=get_S_threshold(risk_list=risk_list)\n",
    "print(rf\"近3、6、9个月风险等级平均值阈值分别为：{mean_3}、{mean_6}、{mean_9}\")\n",
    "print(rf\"近3、6、9个月风险等级平均标准差阈值分别为：{S_3}、{S_6}、{S_9}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_weighting(score_3,score_6,score_9):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成词云\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "万迪羊肉火锅\n",
      "今渡百川\n",
      "仙泷小柒_\n",
      "低级普男\n",
      "你的镜仔气网友版\n",
      "冒牌写手\n",
      "我脑袋瓜嗡嗡的\n",
      "扛不住墙头马上大火\n",
      "是林需药\n",
      "猫猫张圆圆\n",
      "绝对不能被发现的微博小号\n",
      "都好说但是要先给钱\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from wordcloud import WordCloud\n",
    "from HaoChiUtils import DataPreprocess as DP\n",
    "from collections import Counter\n",
    "import os\n",
    "dp=DP(stopwords_file_path=\"stop_words.txt\")\n",
    "def get_wordcloud(folder_path):\n",
    "    folders_list=os.listdir(folder_path)\n",
    "    strs=\" \"\n",
    "    for folder in folders_list:\n",
    "        user_text_list=os.listdir(folder_path+'\\\\'+folder)\n",
    "        print(folder)\n",
    "        for user_text in user_text_list:\n",
    "\n",
    "            file_path=folder_path+'\\\\'+folder+'\\\\'+user_text\n",
    "            with open(file_path,'r',encoding='utf-8') as f:\n",
    "                cnt=0\n",
    "                for line in f:\n",
    "                    cnt+=1\n",
    "                    text=dp.text_clean(line,keep_segmentation=False)\n",
    "                    text=text.split(' ')\n",
    "                    text=[i for i in text if i!='' and len(i)>=2]\n",
    "                    # print(text)\n",
    "                    strs+=' '.join(text)\n",
    "    return strs\n",
    "folder_path=\"制作词云\\正常用户\"\n",
    "seg_text=get_wordcloud(folder_path)\n",
    "# seg_list=seg_text.split(' ')\n",
    "# word_freq = Counter(seg_list)\n",
    "# top_n=50\n",
    "# top_words=dict(word_freq.most_common(top_n))\n",
    "# # 创建词云对象并生成词云\n",
    "# wordcloud = WordCloud(font_path='myttf.ttf', width=800, height=400,collocations=False).generate_from_frequencies(top_words)\n",
    "\n",
    "# # 显示词云\n",
    "# wordcloud.to_image()\n",
    "\n",
    "# # 显示词云\n",
    "# # wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "color=[\"Blues\",\"Reds\",\"Greens\",\"Oranges\",\"Purples\",\"RdYlBu\",\"viridis\",'spring','summer','autumn','winter']\n",
    "seg_list=seg_text.split(' ')\n",
    "word_freq = Counter(seg_list)\n",
    "top_n=80\n",
    "top_words=dict(word_freq.most_common(top_n))\n",
    "# 创建词云对象并生成词云\n",
    "# wordcloud = WordCloud(font_path='FZYTK.ttf',background_color='white', width=800, height=400,collocations=True,colormap='summer',color_func=lambda *args, **kwargs: \"black\").generate_from_frequencies(top_words)\n",
    "for i in range(len(color)):\n",
    "    wordcloud = WordCloud(font_path='myttf.ttf',background_color='black', width=1000, height=600,collocations=True,colormap=color[i]).generate_from_frequencies(top_words)\n",
    "\n",
    "    # 显示词云\n",
    "    # wordcloud.to_image()\n",
    "\n",
    "    # 显示词云\n",
    "    # wordcloud.to_image()\n",
    "    wordcloud.to_file(f\"词云\\正常用户\\\\{i+1}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# wd_0=WordCloud(font_path='myttf.TTF',\n",
    "#                colormap='viridis',\n",
    "#                 width=800,\n",
    "#                 height=400,\n",
    "#                collocations=False\n",
    "#                ).generate(seg_text)\n",
    "# plt.imshow(wd_0,interpolation='bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #将一组文本整合成一个\n",
    "# folder_path=\"用于计算阈值的用户\"\n",
    "# fir_dir=os.listdir(folder_path)\n",
    "# for user in fir_dir:\n",
    "#     with open(f\"{folder_path}\\\\{user}.txt\",'w',encoding='utf-8') as f_write:\n",
    "#         user_path=folder_path+'\\\\'+user \n",
    "#         print(user_path)\n",
    "#         months=os.listdir(user_path)\n",
    "#         for month in months:\n",
    "#             user_text_path=user_path+'\\\\'+month\n",
    "#             with open(user_text_path,'r',encoding='utf-8') as f_read:\n",
    "#                 for line in f_read:\n",
    "#                     line=line.strip()\n",
    "#                     f_write.write(line+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HaoChiUtils import DataAnalyzer as DA\n",
    "from FunctionalInterface import TextEmotionAnalyzer as TEA\n",
    "import os\n",
    "#计算得到占比\n",
    "tea=TEA()\n",
    "file_path=\"计算各项阈值的文本\\用于计算结果1阈值和熵率阈值的用户文本\"\n",
    "users=os.listdir(file_path)\n",
    "for user in users:\n",
    "    user_path=file_path+'\\\\'+user\n",
    "    data_list=DA.get_dataList(user_path,min_len=6)\n",
    "    pro_dict=tea.dri.get_pro_dict(data_list)\n",
    "    print(user)\n",
    "    print(pro_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画折线图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from HaoChiUtils import DataAnalyzer as DA\n",
    "from FunctionalInterface import TextEmotionAnalyzer as TEA\n",
    "import os\n",
    "tea=TEA()\n",
    "folder_path=\"文本集\\测试数据\\8.5\\风险用户\"\n",
    "users=os.listdir(folder_path)\n",
    "for user in users:\n",
    "    user_path=folder_path+'\\\\'+user\n",
    "    risk_rank=tea.risk_rank_list(user_path,min_len=6)\n",
    "    risk_rank=risk_rank[-9:]\n",
    "    for i in range(9-len(risk_rank)):\n",
    "        risk_rank.insert(0,0)\n",
    "    month=[i for i in range(9,0,-1)]\n",
    "    plt.yticks([0,1,2,3])\n",
    "    plt.plot(month,risk_rank,linewidth=3)\n",
    "# Y=[0,1,2,3,2,1,3,1,2]\n",
    "# X=[1,2,3,4,5,6,7,8,9]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
